An extractive summarization algorithm of a single document aims to produce a much shorter text file to cover all the main points of the document without redundancy. Such an algorithm, unsupervised or supervised, extracts a few critical sentences from the document to form a summary.
An unsupervised method typically solves an optimization problem of selecting sentences subject to the underlying length constraint of a summary through sentence scoring and topic diversity. Unsupervised methods have three advantages: (1) They do not need training data; (2) they are in general independent of the underlying languages; and (3) they are much more efficient. Most unsupervised methods use easy-to-compute counting features, such as term frequency-inverse document frequency (TF-IDF) and co-occurrences of words, discarding semantic information. Some methods do incorporate semantic features including Semantic Role Labeling, WordNet, and Named-Entity Recognition when selecting sentences. These methods rely on specific language features which can be a potential problem when adapting across languages.
Supervised methods, on the other hand, may either require handcrafted features or learn feature representations automatically with a deep neural network model trained on a significant amount of data and is often time-consuming.
These considerations motivate us to investigate unsupervised summarization algorithms using semantic features that can be computed readily for any language.
Word embedding representation maps a word into a high-dimensional vector representing continuous relations with other words, which is easy to obtain and does not rely on specific language features. A good pre-trained word embedding representation can help capture both useful semantic and syntactic information. Word Mover's Distance (WMD) utilizes this property to measure the distance between two documents, which brings semantics to sentence similarity. Our method, called Semantic WordRank (SWR), is an unsupervised graph-based algorithm with semantic features for generating extractive single-document summaries. It adds word-embedding representations as semantic relations to a graph with word co-occurrence relations. It then uses an article-structure-biased PageRank algorithm to compute a score for each word and a score for each sentence after an adjustment using the Softplus function. To facilitate coverage diversity, our approach uses a spectral subtopic clustering method to group subtopics under the WMD. Finally, it selects sentences with the highest scores over subtopic clusters in a round-robin fashion until the length constraint of the summary size fails.
The rest of the paper is organized as follows: We present in Section a brief overview of related work on single-document extractive summarizations. Section provide a detailed description of SWR. We describe in Section our evaluation datasets, experiment settings, and comparison results under ROUGE measures on DUC-02 and SummBank datasets. We conclude the paper in Section.
